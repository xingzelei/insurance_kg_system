import streamlit as st
import os
import sys

# Add src to path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, "../../")
sys.path.append(src_path)

from src.rag_engine.rag_pipeline import RAGPipeline

st.set_page_config(page_title="ä¿é™©+åŒ»å…» çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ", layout="wide")

st.sidebar.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")

model_option = st.sidebar.radio(
    "é€‰æ‹©å¤§æ¨¡å‹ç±»å‹",
    ["ZhipuAI API", "Mock æ¨¡æ‹Ÿæ¨¡å¼"],
    index=0  # é»˜è®¤é€‰æ‹© API
)

api_key = None

if model_option == "ZhipuAI API":
    api_key = st.sidebar.text_input("ğŸ”‘ ZhipuAI API Key", type="password", placeholder="è¯·è¾“å…¥æ‚¨çš„ API Key")
    if api_key:
        st.sidebar.success("âœ… å·²é…ç½® API Key")
    else:
        st.sidebar.warning("âš ï¸ è¯·è¾“å…¥ API Key ä»¥ä½¿ç”¨å¤§æ¨¡å‹")
else:
    st.sidebar.info("ğŸ’¡ å½“å‰è¿è¡Œåœ¨ **æ¨¡æ‹Ÿæ¨¡å¼ (Mock)**ï¼Œä»…è¿”å›é¢„è®¾ç­”æ¡ˆã€‚")

@st.cache_resource
def load_pipeline(key):
    # Path relative to where we run streamlit
    base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    data_path = os.path.join(base_path, "data/processed")
    return RAGPipeline(data_path, api_key=key)

try:
    pipeline = load_pipeline(api_key)
except FileNotFoundError:
    st.error("Knowledge Graph not found. Please run 'src/kg_construction/graph_builder.py' first.")
    st.stop()

st.title("ğŸ¥ ä¿é™©+åŒ»å…» è·¨åŸŸçŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ")

col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
    example = st.radio("é€‰æ‹©ä¸€ä¸ªé—®é¢˜:", 
             ["é«˜è¡€å‹èƒ½ä¹°ä»€ä¹ˆä¿é™©ï¼Ÿ", 
              "æ³°åº·ä¹‹å®¶Â·ç‡•å›­åœ¨å“ªé‡Œï¼Ÿ", 
              "æ³°åº·å…¨èƒ½ä¿è¦†ç›–ä»€ä¹ˆç–¾ç—…ï¼Ÿ"])

with col2:
    st.subheader("ğŸ’¬ å¯¹è¯äº¤äº’")
    user_input = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:", value=example)
    
    if st.button("æé—®"):
        if user_input:
            # 1. Retrieval Phase
            with st.status("ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†å›¾è°±...", expanded=True) as status:
                st.write("æ­£åœ¨æœç´¢ç›¸å…³å®ä½“...")
                # Split the pipeline call to show progress
                context = pipeline.retriever.get_context(user_input, hops=1)
                st.write("æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ä¸Šä¸‹æ–‡ã€‚")
                status.update(label="âœ… å›¾è°±æ£€ç´¢å®Œæˆ", state="complete", expanded=False)

            # 2. Generation Phase
            with st.spinner("ğŸ¤– æ¨¡å‹æ­£åœ¨æ€è€ƒ (æœ¬åœ°è¿è¡Œé€Ÿåº¦å–å†³äºç¡¬ä»¶é…ç½®)..."):
                # Construct prompt manually (replicating logic from pipeline to decouple UI)
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¿é™©åŒ»å…»åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
                
                ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
                {context}
                
                ç”¨æˆ·é—®é¢˜ï¼š{user_input}
                
                å›ç­”è¦æ±‚ï¼šå‡†ç¡®ï¼ŒåŸºäºäº‹å®ï¼Œå¼•ç”¨ä¸Šä¸‹æ–‡ã€‚
                """
                
                answer = pipeline.llm.generate(prompt)
                
                st.success("å›ç­”ç”ŸæˆæˆåŠŸ")
                st.markdown(f"### ğŸ¤– å›ç­”\n{answer}")
                
                with st.expander("æŸ¥çœ‹çŸ¥è¯†å›¾è°±è¯æ® (Context)"):
                    st.text(context)
